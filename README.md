This repository serves as the implementation space for my ongoing bachelor thesis

Pre-title
""Exploring the hidden structure of Attention Layers: An explorative study of patterns in attention layers in Language and Vision Transformer Models through the lens of Gaussian Distributions""

Pre-Abstract:
This thesis presents an in-depth analysis of the structural properties of attention maps in transformer models, employing a novel approach by analyzing these properties through the lens of Gaussian distributions derived from Principal Component Analysis (PCA) of the attention maps. By examining the singular value spectrum and ellipsoid orientation, this study seeks to uncover underlying patterns and differences among attention heads and layers within transformer architectures. To quantitatively compare the Gaussian distributions associated with different attention mechanisms, the Kullback-Leibler (KL) divergence is utilized as a measure of structural divergence between these distributions.
