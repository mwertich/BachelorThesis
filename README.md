This repository serves as the implementation space for my ongoing bachelor thesis

Pre-title
"Exploring the structure of Attention Layers: An explorative and comparative Study of Attention Structures in Language and Vision Transformer Models through the lens of Gaussian Distributions(PCA)"

Pre-Abstract:
This thesis presents an in-depth analysis of the structural properties of attention maps in transformer models, employing a novel approach by analyzing these properties through the lens of Gaussian distributions derived from Principal Component Analysis (PCA) of the attention maps. By examining the singular value spectrum and ellipsoid orientation, this study seeks to uncover underlying patterns and differences among attention heads and layers within transformer architectures. To quantitatively compare the Gaussian distributions associated with different attention mechanisms, the Kullback-Leibler (KL) divergence is utilized as a measure of structural divergence between these distributions.

A comprehensive evaluation is conducted across various domains, including Computer Vision and NLP, focusing on both self-attention and cross-attention mechanisms for text, image data, and their combinations. The study meticulously explores the relationships between the eigenvalue decompositions of the attention maps, the contrasting KL divergence values, and their relevance to the task domain. The objective is to establish connections between the mathematical characteristics of attention map structures and their functional implications in diverse task domains.

By bridging the gap between theoretical mathematical constructs and practical application scenarios, this thesis aims to provide insights into how transformer models manage attention across different layers and heads, and how these mechanisms can be interpreted and leveraged in the context of specific computational tasks. This work contributes to a deeper understanding of the operational dynamics of transformer models, offering a foundation for future research into optimizing and interpreting these powerful neural network architectures.
